# remote-model-access
Allows you to chat with a llm from a separate device on the same wifi on an iphone.
This was made in Xcode, and I dont have an apple developer account so I cant make this an .ipa, unless I find some other way.
Ill put more info and files here soon
Originally this was to be used with tailscale but im not sure if tailscale will help.
# how to use
ğŸ”° Download the zip file in RELEASES.

ğŸ”° After unzipping, open it with Xcode.

ğŸ”° Pair your iPhone with Xcode.

ğŸ”° Select your iPhone as a run destination and run it.

ğŸ”° The app should then download to your iPhone.

ğŸ”° In the app, put it in at least the Server Endpoint and API Key.

ğŸ”° Now you should be good, as long as you're connected through wireguard (I used tailscale) or on the same wifi.

## I used LM Studio to host a llm, but you can use ollama or others as long as they give you a working server endpoint and api key.

# features (this is kind of simple but still)
â¤ï¸â€ğŸ”¥ Connect to an llm running locally on your computer from your âœ¨iphoneâœ¨

â¤ï¸â€ğŸ”¥ Make multiple chats
 
 # coming soon (eventually) (not in order)
ğŸ‘€ Video turorial

ğŸ‘€ Apple watch app

ğŸ‘€ Renaming chats

ğŸ‘€ Send images

ğŸ‘€ Loading indicator

ğŸ‘€ Multiple models

ğŸ‘€ Apple TV app (Maybe)

ğŸ‘€ Saving llm presets (Maybe)
